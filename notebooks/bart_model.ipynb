{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ce9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class EntailmentMemory(nn.Module):\n",
    "    def __init__(self, num_slots: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.num_slots = num_slots\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.memory = nn.Parameter(torch.randn(num_slots, hidden_dim))  # [K, D]\n",
    "        self.proj = nn.Linear(hidden_dim, num_slots)  # Maps BART hidden states to memory slots\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_state: [batch_size, hidden_dim] (e.g., BART's [z] token embedding)\n",
    "        Returns:\n",
    "            z: Entailment representation [batch_size, hidden_dim]\n",
    "            attn_weights: Memory attention scores [batch_size, num_slots]\n",
    "        \"\"\"\n",
    "        attn_weights = torch.softmax(self.proj(hidden_state), dim=-1)  # [batch_size, K]\n",
    "        z = torch.einsum('bk,kd->bd', attn_weights, self.memory)      # [batch_size, D]\n",
    "        return z, attn_weights\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16c5af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class DiscourseMemory(nn.Module):\n",
    "    def __init__(self, num_slots: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.num_slots = num_slots\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.memory = nn.Parameter(torch.randn(num_slots, hidden_dim))  # [L, D]\n",
    "        self.proj = nn.Linear(hidden_dim, num_slots)\n",
    "\n",
    "    def forward(self, hidden_state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Same as ERM but for discourse features.\"\"\"\n",
    "        attn_weights = torch.softmax(self.proj(hidden_state), dim=-1)  # [batch_size, L]\n",
    "        z_d = torch.einsum('bl,ld->bd', attn_weights, self.memory)     # [batch_size, D]\n",
    "        return z_d, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e494144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "import torch.nn as nn\n",
    "from transformers import BartForConditionalGeneration, BartConfig\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "class BARTWithMemory(nn.Module):\n",
    "    def __init__(self, \n",
    "                 bart_model_name: str = \"facebook/bart-base\",\n",
    "                 erm_slots: int = 10, \n",
    "                 ddm_slots: int = 5):\n",
    "        super().__init__()\n",
    "        # Load full model to preserve components\n",
    "        full_bart = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
    "        self.bart = full_bart.model\n",
    "        self.lm_head = full_bart.lm_head\n",
    "        self.final_logits_bias = full_bart.final_logits_bias\n",
    "        self.config: BartConfig = full_bart.config\n",
    "        # self.generate = full_bart.generate\n",
    "\n",
    "        # Initialize memory modules\n",
    "        self.erm = EntailmentMemory(erm_slots, self.config.d_model)\n",
    "        self.ddm = DiscourseMemory(ddm_slots, self.config.d_model)\n",
    "        self.ortho_loss_coeff = 0.1\n",
    "\n",
    "        # Register token IDs from config\n",
    "        self.register_buffer(\"sop_token_id\", torch.tensor([self.config.bos_token_id]))\n",
    "        self.register_buffer(\"eop_token_id\", torch.tensor([self.config.eos_token_id]))\n",
    "\n",
    "    def orthogonal_loss(self) -> torch.Tensor:\n",
    "        \"\"\"Orthogonality constraint between memory matrices\"\"\"\n",
    "        return torch.norm(torch.mm(self.erm.memory, self.ddm.memory.T)) ** 2\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        encoder_outputs: Optional[Tuple[torch.FloatTensor]] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        decoder_head_mask: Optional[torch.FloatTensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.FloatTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs\n",
    "    ) -> Seq2SeqLMOutput:\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # Encode inputs\n",
    "        if encoder_outputs is None:\n",
    "            encoder_outputs = self.bart.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "        \n",
    "        last_hidden = encoder_outputs.last_hidden_state\n",
    "\n",
    "        # Memory operations\n",
    "        z_token_embedding = last_hidden[:, 0]  # First token embedding\n",
    "        z, _ = self.erm(z_token_embedding)\n",
    "        z_d, _ = self.ddm(z_token_embedding)\n",
    "\n",
    "        # Prepare decoder inputs\n",
    "        if labels is not None:\n",
    "            if decoder_input_ids is None:\n",
    "                decoder_input_ids = self._shift_right(labels)\n",
    "        \n",
    "        # Embed decoder inputs\n",
    "        decoder_inputs_embeds = self.bart.decoder.embed_tokens(decoder_input_ids)\n",
    "        decoder_inputs_embeds[:, 0] += z + z_d  # Modify first token\n",
    "\n",
    "        # Decode\n",
    "        decoder_outputs = self.bart.decoder(\n",
    "            input_ids=None,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            encoder_hidden_states=last_hidden,\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # Calculate logits and losses\n",
    "        lm_logits = self.lm_head(decoder_outputs.last_hidden_state) + self.final_logits_bias\n",
    "        loss = None\n",
    "        ortho_loss = self.orthogonal_loss()\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=self.config.pad_token_id)\n",
    "            ce_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            loss = ce_loss + self.ortho_loss_coeff * ortho_loss\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "    def _shift_right(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Shift labels to create decoder inputs (like original BART)\"\"\"\n",
    "        shifted = input_ids.new_zeros(input_ids.shape)\n",
    "        shifted[:, 1:] = input_ids[:, :-1].clone()\n",
    "        shifted[:, 0] = self.sop_token_id\n",
    "        return shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95a5d6e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BARTWithMemory' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m input_ids = tokenizer(\u001b[33m\"\u001b[39m\u001b[33mHello, how are you?\u001b[39m\u001b[33m\"\u001b[39m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).input_ids\n\u001b[32m      5\u001b[39m labels = tokenizer(\u001b[33m\"\u001b[39m\u001b[33mI am fine, thank you.\u001b[39m\u001b[33m\"\u001b[39m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).input_ids\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m(input_ids=input_ids, labels=labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rk225\\Documents\\Project_College\\college-chatbot-v3\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'BARTWithMemory' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "from transformers import BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model = BARTWithMemory()\n",
    "input_ids = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").input_ids\n",
    "labels = tokenizer(\"I am fine, thank you.\", return_tensors=\"pt\").input_ids\n",
    "model.generate(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5500db62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "model = BARTWithMemory()\n",
    "\n",
    "# Structured example\n",
    "input_text = \"<latent><persona>p1,p2<query>q<response>\"\n",
    "output_text = \"r<eos>\"\n",
    "\n",
    "# Tokenize (make sure to add special tokens to your tokenizer first!)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "labels = tokenizer(output_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    labels=labels\n",
    ")\n",
    "\n",
    "loss = outputs.loss\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e153da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3052.6052, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4715f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<latent> <persona> Hi I am College Buddy <query> WHat is Your Name <responce> \n"
     ]
    }
   ],
   "source": [
    "# Generate response\n",
    "\n",
    "inputs = tokenizer(\"<latent> <persona> Hi I am College Buddy <query> WHat is Your Name <responce> \", return_tensors=\"pt\")\n",
    "decoder_input_ids = tokenizer(\"<latent> <persona> Hi I am College Buddy <query> WHat is Your Name <responce> \", return_tensors=\"pt\").input_ids\n",
    "output_ids = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    decoder_input_ids=decoder_input_ids,\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.7,\n",
    "    top_k=50,\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and extract response\n",
    "full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "# response = full_output.split(\"<response>\")[-1].split(\"<eos>\")[0].strip()\n",
    "print(full_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cbac0c",
   "metadata": {},
   "source": [
    "### NExt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0d0b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "import torch.nn as nn\n",
    "from transformers import BartForConditionalGeneration, BartConfig, BartModel\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "class EntailmentMemory(nn.Module):\n",
    "    def __init__(self, num_slots: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.memory = nn.Parameter(torch.randn(num_slots, embedding_dim))\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # x: [batch_size, embedding_dim]\n",
    "        projected = self.proj(x)  # [batch_size, embedding_dim]\n",
    "        scores = torch.matmul(projected, self.memory.T)  # [batch_size, num_slots]\n",
    "        weights = self.softmax(scores)\n",
    "        output = torch.matmul(weights, self.memory)  # [batch_size, embedding_dim]\n",
    "        return output, weights\n",
    "\n",
    "class DiscourseMemory(nn.Module):\n",
    "    def __init__(self, num_slots: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.memory = nn.Parameter(torch.randn(num_slots, embedding_dim))\n",
    "        self.proj = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        projected = self.proj(x)\n",
    "        scores = torch.matmul(projected, self.memory.T)\n",
    "        weights = self.softmax(scores)\n",
    "        output = torch.matmul(weights, self.memory)\n",
    "        return output, weights\n",
    "\n",
    "class BartModelWithMemory(BartModel):\n",
    "    def __init__(self, config: BartConfig, erm_slots: int = 10, ddm_slots: int = 5):\n",
    "        super().__init__(config)\n",
    "        self.erm = EntailmentMemory(erm_slots, config.d_model)\n",
    "        self.ddm = DiscourseMemory(ddm_slots, config.d_model)\n",
    "        self.ortho_loss_coeff = 0.1\n",
    "\n",
    "    def orthogonal_loss(self) -> torch.Tensor:\n",
    "        return torch.norm(torch.mm(self.erm.memory, self.ddm.memory.T)) ** 2\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        # Original BART forward\n",
    "        outputs = super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            decoder_head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # Memory operations only on initial forward pass\n",
    "        if past_key_values is None:\n",
    "            encoder_hidden = outputs.encoder_last_hidden_state\n",
    "            z_token = encoder_hidden[:, 0]  # Use first token embedding\n",
    "            \n",
    "            # Get memory outputs\n",
    "            erm_out, _ = self.erm(z_token)\n",
    "            ddm_out, _ = self.ddm(z_token)\n",
    "            \n",
    "            # Modify decoder inputs\n",
    "            if decoder_inputs_embeds is None:\n",
    "                decoder_inputs_embeds = self.decoder.embed_tokens(decoder_input_ids)\n",
    "            \n",
    "            # Apply memory to decoder's first token embedding\n",
    "            decoder_inputs_embeds[:, 0] += erm_out + ddm_out\n",
    "            \n",
    "            # Re-run decoder with modified inputs\n",
    "            outputs = super().forward(\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "            )\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class BARTWithMemory(BartForConditionalGeneration):\n",
    "    def __init__(self, config: BartConfig, erm_slots: int = 10, ddm_slots: int = 5):\n",
    "        super().__init__(config)\n",
    "        self.model = BartModelWithMemory(config, erm_slots, ddm_slots)\n",
    "        self.post_init()  # Important for loading pretrained weights\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        outputs = super().forward(**kwargs)\n",
    "        \n",
    "        # Add orthogonal loss\n",
    "        if kwargs.get('labels') is not None:\n",
    "            ortho_loss = self.model.orthogonal_loss()\n",
    "            outputs.loss += self.model.ortho_loss_coeff * ortho_loss\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dc99ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BARTWithMemory were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['model.ddm.memory', 'model.ddm.proj.bias', 'model.ddm.proj.weight', 'model.erm.memory', 'model.erm.proj.bias', 'model.erm.proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text:\n",
      "The quick brown fox jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    from transformers import BartTokenizer\n",
    "\n",
    "    # Initialize model\n",
    "    config = BartConfig.from_pretrained(\"facebook/bart-base\")\n",
    "    model = BARTWithMemory.from_pretrained(\n",
    "        \"facebook/bart-base\",\n",
    "        config=config,\n",
    "        erm_slots=10,\n",
    "        ddm_slots=5\n",
    "    )\n",
    "    tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "\n",
    "    # Sample generation\n",
    "    text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate with memory\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=100,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nGenerated text:\")\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0f0318",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
